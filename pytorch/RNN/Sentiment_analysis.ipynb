{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load in and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                              text\n",
      "0      1   Absolutely loved this product, would buy again!\n",
      "1      0               Waste of money, completely useless.\n",
      "2      1              This phone exceeded my expectations.\n",
      "3      0  The battery died after two hours, disappointing.\n",
      "4      1                 Great service and friendly staff.\n",
      "Reviews:\n",
      "                                               text\n",
      "0   Absolutely loved this product, would buy again!\n",
      "1               Waste of money, completely useless.\n",
      "2              This phone exceeded my expectations.\n",
      "3  The battery died after two hours, disappointing.\n",
      "4                 Great service and friendly staff. \n",
      "\n",
      "Labels:\n",
      "   label\n",
      "0      1\n",
      "1      0\n",
      "2      1\n",
      "3      0\n",
      "4      1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"mini_sentiment_dataset.txt\",\n",
    "                 sep=\"\\t\", names=[\"label\", \"text\"])\n",
    "print(df.head())\n",
    "\n",
    "# --- Split into two datasets ------------------------------------\n",
    "reviews = df[[\"text\"]].copy()    # DataFrame with a single 'text' column\n",
    "labels  = df[[\"label\"]].copy()   # DataFrame with a single 'label' column\n",
    "\n",
    "# Optional: inspect them\n",
    "print(\"Reviews:\")\n",
    "print(reviews.head(), \"\\n\")\n",
    "\n",
    "print(\"Labels:\")\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.\n",
    "\n",
    "You can see an example of the reviews data above. Here are the processing steps, we'll want to take:\n",
    ">* We'll want to get rid of periods and extraneous punctuation.\n",
    "* Also, you might notice that the reviews are delimited with newline characters `\\n`. To deal with those, I'm going to split the text into each review using `\\n` as the delimiter. \n",
    "* Then I can combined all the reviews back together into one big string.\n",
    "\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word\n",
      "0           a\n",
      "1  absolutely\n",
      "2      acting\n",
      "3       after\n",
      "4       again\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ── 1.  Lower‑case everything and strip out non‑letters ───────────\n",
    "clean_words = (\n",
    "    reviews['text']                         # the column with full sentences\n",
    "      .str.lower()                          # lower‑case\n",
    "      .str.replace(r'[^a-z\\s]', ' ', regex=True)  # keep only a–z and spaces\n",
    "      .str.split()                          # split on whitespace → lists of words\n",
    "      .explode()                            # one word per row\n",
    ")\n",
    "\n",
    "# ── 2.  Drop any empty tokens, get uniques, and build the vocab DF ──\n",
    "words = (\n",
    "    clean_words[clean_words != '']          # remove blanks\n",
    "      .drop_duplicates()\n",
    "      .sort_values()\n",
    "      .reset_index(drop=True)\n",
    "      .to_frame(name='word')                # single‑column DataFrame\n",
    ")\n",
    "\n",
    "print(words.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "> **Exercise:** Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers **start at 1, not 0**.\n",
    "> Also, convert the reviews to integers and store the reviews in a new list called `reviews_ints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 54, 80, 66, 93, 18, 5],\n",
       " [88, 60, 56, 20, 84],\n",
       " [80, 62, 30, 58, 31],\n",
       " [79, 15, 26, 4, 81, 44, 27],\n",
       " [41, 73, 10, 40, 76],\n",
       " [71, 76, 10, 78, 73],\n",
       " [46, 7, 85, 63, 91, 79, 67],\n",
       " [33, 28, 70, 51],\n",
       " [9, 8, 32, 61],\n",
       " [59, 92, 79, 45, 14, 6],\n",
       " [35, 74, 10, 52, 13, 25],\n",
       " [52, 12, 17, 10, 53],\n",
       " [79, 57, 87, 34, 43, 69],\n",
       " [16, 64, 10, 65, 3],\n",
       " [79, 11, 72, 82, 49, 48, 10, 75],\n",
       " [22, 21, 83],\n",
       " [24, 55, 90, 86, 5],\n",
       " [38, 87, 19, 10, 37],\n",
       " [42, 23, 77, 36, 58, 50, 68],\n",
       " [77, 47, 58, 29, 39, 1, 89]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(words['word'])}\n",
    "word2idx\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "# Tokenise every review using that dictionary  ───────────────\n",
    "# reviews_ints will be a pandas Series of integer lists\n",
    "reviews_ints = (\n",
    "    reviews['text']\n",
    "        .str.lower()\n",
    "        .str.replace(r'[^a-z\\s]', ' ', regex=True)\n",
    "        .str.split()\n",
    "        .apply(lambda toks: [word2idx[w] for w in toks if w in word2idx])\n",
    ")\n",
    "\n",
    "# Optional: convert to plain Python list for downstream code\n",
    "reviews_ints = reviews_ints.tolist()\n",
    "reviews_ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels\n",
    "\n",
    "Our labels are \"positive\" or \"negative\". To use these labels in our network, we need to convert them to 0 and 1.\n",
    "\n",
    "> **Exercise:** Convert labels from `positive` and `negative` to 1 and 0, respectively, and place those in a new list, `encoded_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = labels['label'].astype(int).tolist()   # <- plain Python list\n",
    "# quick check\n",
    "print(encoded_labels[:5])        # [1, 0, 1, 0, 1]  etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 8\n"
     ]
    }
   ],
   "source": [
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "# we are good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some `seq_length`, we'll pad with 0s. For reviews longer than `seq_length`, we can truncate them to the first `seq_length` words. A good `seq_length`, in this case, is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape : (20, 200)\n",
      "First row (non‑zero tail):\n",
      " [ 0  0  0  2 54 80 66 93 18  5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_features(reviews_ints, seq_length=200):  \n",
    "    n_reviews = len(reviews_ints)\n",
    "    \n",
    "    # Start with a zero‑matrix; dtype=int for PyTorch or TensorFlow later.\n",
    "    features = np.zeros((n_reviews, seq_length), dtype=np.int32)\n",
    "    \n",
    "    for i, review in enumerate(reviews_ints):\n",
    "        # Use only the first `seq_length` tokens if review is too long\n",
    "        review_slice = review[:seq_length]\n",
    "        \n",
    "        # Place tokens at the END of the row (left‑pad with 0s)\n",
    "        features[i, -len(review_slice):] = review_slice\n",
    "\n",
    "    return features\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Example usage\n",
    "# ------------------------------------------------------------------\n",
    "seq_length = 200\n",
    "features = pad_features(reviews_ints, seq_length)\n",
    "\n",
    "print(\"Feature matrix shape :\", features.shape)   # (num_reviews, 200)\n",
    "print(\"First row (non‑zero tail):\\n\", features[0][-10:])  # peek last 10 positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test\n",
    "\n",
    "## DataLoaders and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tFeature Shapes\n",
      "Train : (16, 200) \n",
      "Valid : (2, 200) \n",
      "Test  : (2, 200)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0) Make sure labels are a NumPy array with shape (N,)\n",
    "# ------------------------------------------------------------------\n",
    "labels_np = np.asarray(encoded_labels, dtype=np.int64)    # <- 1‑D\n",
    "\n",
    "# `features` is already the padded NumPy array from pad_features()\n",
    "# features.shape  ->  (N, seq_length)\n",
    "features_np = features.astype(np.int64)                   # keep dtype ints\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Train / validation / test split\n",
    "# ------------------------------------------------------------------\n",
    "split_frac = 0.8\n",
    "\n",
    "split_idx      = int(len(features_np) * split_frac)\n",
    "train_x        = features_np[:split_idx]\n",
    "train_y        = labels_np[:split_idx]\n",
    "\n",
    "remaining_x    = features_np[split_idx:]\n",
    "remaining_y    = labels_np[split_idx:]\n",
    "\n",
    "test_idx       = int(len(remaining_x) * 0.5)\n",
    "val_x, test_x  = remaining_x[:test_idx],  remaining_x[test_idx:]\n",
    "val_y, test_y  = remaining_y[:test_idx],  remaining_y[test_idx:]\n",
    "\n",
    "print(\"\\n\\t\\tFeature Shapes\")\n",
    "print(\"Train :\", train_x.shape,\n",
    "      \"\\nValid :\", val_x.shape,\n",
    "      \"\\nTest  :\", test_x.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Wrap splits in TensorDataset objects\n",
    "# ------------------------------------------------------------------\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),\n",
    "                           torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x),\n",
    "                           torch.from_numpy(val_y))\n",
    "test_data  = TensorDataset(torch.from_numpy(test_x),\n",
    "                           torch.from_numpy(test_y))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Build DataLoaders (shuffle the training set)\n",
    "# ------------------------------------------------------------------\n",
    "batch_size  = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True,  batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_data,  shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([1, 200])\n",
      "Sample input: \n",
      " tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 88, 60, 56,\n",
      "         20, 84]])\n",
      "\n",
      "Sample label size:  torch.Size([1])\n",
      "Sample label: \n",
      " tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out[:, -1, :] # getting the last time step output\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(94, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(word2idx)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(1), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(1), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "There are a few ways to test your network.\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n",
    "\n",
    "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called **inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.557\n",
      "Test accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(1), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze(1))\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
