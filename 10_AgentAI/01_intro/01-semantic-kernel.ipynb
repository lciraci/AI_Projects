{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel \n",
    "\n",
    "In this code sample, you will use the [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel) AI Framework to create a basic agent. \n",
    "\n",
    "The goal of this sample is to show you the steps that we will later use in the additional code samples when implementing the different agentic patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Needed Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import Annotated\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.functions import kernel_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Client\n",
    "\n",
    "In this sample, we will use [GitHub Models](https://aka.ms/ai-agents-beginners/github-models) for access to the LLM. \n",
    "\n",
    "The `ai_model_id` is defined as `gpt-4o-mini`. Try changing the model to another model available on the GitHub Models marketplace to see the different results. \n",
    "\n",
    "For us to use the `Azure Inference SDK` that is used for the `base_url` for GitHub Models, we will use the `OpenAIChatCompletion` connector within Semantic Kernel. There are also other [available connectors](https://learn.microsoft.com/semantic-kernel/concepts/ai-services/chat-completion) to use Semantic Kernel for other model providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Annotated\n",
    "\n",
    "# Define a sample plugin for dinner suggestions\n",
    "\n",
    "class DinnerPlugin:\n",
    "    \"\"\"A list of random Italian dishes for dinner with calorie information.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # List of Italian dishes with estimated calories\n",
    "        self.dishes = [\n",
    "            \"Pizza Margherita - approx. 300 kcal/slice\",\n",
    "            \"Spaghetti alla Carbonara - approx. 600 kcal/portion\",\n",
    "            \"Eggplant Parmigiana - approx. 450 kcal/portion\",\n",
    "            \"Grilled Salmon with Broccoli - approx. 500 kcal/portion\",\n",
    "            \"Minestrone Soup - approx. 200 kcal/bowl\",\n",
    "            \"Chicken Cacciatore - approx. 400 kcal/portion\",\n",
    "            \"Caprese Salad - approx. 250 kcal/plate\",\n",
    "            \"Lasagna alla Bolognese - approx. 650 kcal/portion\",\n",
    "            \"Risotto ai Funghi - approx. 500 kcal/portion\",\n",
    "            \"Orecchiette con Cime di Rapa - approx. 450 kcal/portion\"\n",
    "        ]\n",
    "        # Track last suggestion to avoid repeats\n",
    "        self.last_dish = None\n",
    "\n",
    "    @kernel_function(description=\"Provides a random Italian dinner suggestion.\")\n",
    "    def get_random_dinner(self) -> Annotated[str, \"Returns a random Italian dinner dish with calories.\"]:\n",
    "        # Get available dishes excluding the last one, if possible\n",
    "        available_dishes = self.dishes.copy()\n",
    "        if self.last_dish and len(available_dishes) > 1:\n",
    "            available_dishes.remove(self.last_dish)\n",
    "\n",
    "        # Select a random dish\n",
    "        dish = random.choice(available_dishes)\n",
    "\n",
    "        # Update the last suggested dish\n",
    "        self.last_dish = dish\n",
    "\n",
    "        return dish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"), \n",
    "    base_url=\"https://models.inference.ai.azure.com/\",\n",
    ")\n",
    "\n",
    "# Create an AI Service that will be used by the `ChatCompletionAgent`\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agent \n",
    "\n",
    "Below we create the Agent called `TravelAgent`.\n",
    "\n",
    "For this example, we are using very simple instructions. You can change these instructions to see how the agent responds differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service, \n",
    "    plugins=[DinnerPlugin()],\n",
    "    name=\"DinnerSuggestionAgent\",\n",
    "    instructions=\"You are a helpful AI Agent that can help choose a random Italian dinner dish with calorie information. Explaining how to make the dish\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "Now we can run the Agent by defining a thread of type `ChatHistoryAgentThread`.  Any required system messages are provided to the agent's invoke_stream `messages` keyword argument.\n",
    "\n",
    "After these are defined, we create a `user_inputs` that will be what the user is sending to the agent. In this case, we have set this message to `Plan me a sunny vacation`. \n",
    "\n",
    "Feel free to change this message to see how the agent responds differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: Plan me a dinner for tonight.\n",
      "\n",
      "# DinnerSuggestionAgent: How about making **Orecchiette con Cime di Rapa** for dinner tonight? This dish is delicious and has approximately 450 calories per portion. \n",
      "\n",
      "### Ingredients:\n",
      "- 400g orecchiette pasta\n",
      "- 500g cime di rapa (broccoli rabe)\n",
      "- 3 cloves garlic, minced\n",
      "- 1/4 tsp red pepper flakes (adjust to taste)\n",
      "- 4 tbsp olive oil\n",
      "- Salt to taste\n",
      "- Grated Parmesan cheese (optional)\n",
      "\n",
      "### Instructions:\n",
      "\n",
      "1. **Prepare the Greens**: Start by cleaning the cime di rapa. Remove the tough stems, and rinse them thoroughly under cold water. \n",
      "\n",
      "2. **Cook the Pasta**: In a large pot, bring salted water to a boil and add the orecchiette. Cook according to package instructions until al dente.\n",
      "\n",
      "3. **Blanch the Cime di Rapa**: In the last 3-4 minutes of pasta cooking time, add the cime di rapa to the pot. Let it cook with the pasta until both are tender.\n",
      "\n",
      "4. **Sauté the Garlic**: Meanwhile, in a large skillet, heat the olive oil over medium heat. Add the minced garlic and red pepper flakes, sautéing until the garlic becomes fragrant but not browned.\n",
      "\n",
      "5. **Combine**: Once the pasta and greens are done, drain them and reserve some of the pasta water. Add the orecchiette and cime di rapa to the skillet with garlic. Toss to combine, adding a splash of reserved pasta water if needed to help the sauce coat the pasta.\n",
      "\n",
      "6. **Serve**: Taste and season with salt as needed. Serve the dish hot, topped with grated Parmesan cheese if desired.\n",
      "\n",
      "Enjoy your Orecchiette con Cime di Rapa!\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    # Create a new thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "    user_inputs = [\n",
    "        \"Plan me a dinner for tonight.\",\n",
    "    ]\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        print(f\"# User: {user_input}\\n\")\n",
    "        first_chunk = True\n",
    "        async for response in agent.invoke_stream(\n",
    "            messages=user_input, thread=thread,\n",
    "        ):\n",
    "            # 5. Print the response\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(f\"{response}\", end=\"\", flush=True)\n",
    "            thread = response.thread\n",
    "        print()\n",
    "\n",
    "    # Clean up the thread\n",
    "    await thread.delete() if thread else None\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
